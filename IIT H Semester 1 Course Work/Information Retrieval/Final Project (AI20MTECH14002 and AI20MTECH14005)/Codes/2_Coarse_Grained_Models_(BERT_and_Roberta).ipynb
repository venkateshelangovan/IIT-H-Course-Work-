{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coarse Grained Models (BERT and Roberta).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQXnVz7ME4YF"
      },
      "source": [
        "# Code Objective:\n",
        "\n",
        "*   BERT Model for Coarse Grained Evaluation\n",
        "*   XLMRoBerta Model for Coarse Grained Evaluation\n",
        "*   Saving Probabilities (BERT and XLMRoberta) for Ensemble\n",
        "\n",
        "# Code Result:\n",
        "*   Accuracy - BERT Model for Coarse Grained Evaluation = 91.63 %\n",
        "*   Accuracy - XLMRoBerta for Coarse Grained Evaluation = 89.76 %\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXnV9dwF50Yd"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvMlDjs63TBG"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_ppFOaeJxHD"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json, re\n",
        "from tqdm import tqdm_notebook\n",
        "from uuid import uuid4\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import glue_compute_metrics\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertModel, BertConfig, BertForSequenceClassification\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel, XLMRobertaConfig, XLMRobertaForSequenceClassification\n",
        "\n",
        "print(\"GPU Torch Available = {}\".format(torch.cuda.is_available()))\n",
        "print(\"Torch Version = {}\".format(torch.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stY-Wn8U56mT"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDZqu8NCXxeL"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlZhZHC9YG4a"
      },
      "source": [
        "# Loading Training Data\n",
        "file = '/content/drive/My Drive/CONSTRAINT 2021 Projects (AAAI)/Hindi_Task/Dataset/NER_binary_train.xlsx'\n",
        "train_df = pd.read_excel(file)\n",
        "train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kpq87FQrYLQg"
      },
      "source": [
        "# Loading Validation Data\n",
        "file = '/content/drive/My Drive/CONSTRAINT 2021 Projects (AAAI)/Hindi_Task/Dataset/NER_binary_validate.xlsx'\n",
        "test_df = pd.read_excel(file)\n",
        "test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPPD7q2_YM55"
      },
      "source": [
        "# Data Preparation into Pandas Dataframe for Model Input\n",
        "def get_data(a):\n",
        "  Unique_ID = list(a['Unique ID'])\n",
        "  sentence = list(a['Post'])\n",
        "  text_labels = list(a['Labels Set'])\n",
        "\n",
        "  label = []\n",
        "  for i in text_labels:\n",
        "    if i=='non-hostile':\n",
        "        label.append(0)\n",
        "    elif i=='hostile':\n",
        "        label.append(1)\n",
        "\n",
        "  raw_data_train = {'UID':Unique_ID,'sentence': sentence, 'label': label}\n",
        "  df = pd.DataFrame(raw_data_train, columns = ['UID','sentence','label'])\n",
        "  return df\n",
        "\n",
        "train_data = get_data(train_df)\n",
        "test_data  = get_data(test_df)\n",
        "\n",
        "\n",
        "print(train_data[0:3])\n",
        "print(test_data[0:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKYQMf1D6KhK"
      },
      "source": [
        "# Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ALqUM9f5Qnx"
      },
      "source": [
        "# Choose and Load Model\n",
        "model_name = 'Bert'\n",
        "\n",
        "if (model_name == 'Bert'):\n",
        "  # Bert Parameters\n",
        "  config = BertConfig.from_pretrained('bert-base-multilingual-cased',num_labels=2)\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "  model = BertForSequenceClassification(config)\n",
        "elif (model_name = 'Roberta'):\n",
        "  # XLMRoberta Parameters\n",
        "  config = XLMRobertaConfig.from_pretrained('xlm-roberta-base',num_labels=2)\n",
        "  tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "  model = XLMRobertaForSequenceClassification(config)\n",
        "else:\n",
        "  print('Choose correct Model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP_wD20y8XmK"
      },
      "source": [
        "# Data Preparation for Model Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf0ATRkF5LEx"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.sentence = dataframe.sentence\n",
        "        self.targets = self.data.label\n",
        "        self.max_len = max_len\n",
        "    def __len__(self):\n",
        "        return len(self.sentence)\n",
        "    def __getitem__(self, index):\n",
        "        sentence1 = str(self.sentence[index])\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(sentence1,\n",
        "                                            truncation=True,\n",
        "                                            add_special_tokens=True,\n",
        "                                            max_length=self.max_len,\n",
        "                                            pad_to_max_length=True,\n",
        "                                            return_token_type_ids=True)\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        return {'input_ids': torch.tensor(ids, dtype=torch.long),\n",
        "                'attention_mask': torch.tensor(mask, dtype=torch.long),\n",
        "                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "                'labels': torch.tensor(self.targets[index], dtype=torch.long)\n",
        "               }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDx-QxmX5Muc"
      },
      "source": [
        "# Dataset for Input into Model\n",
        "MAX_LEN = 128                                                # Max Sequence Length\n",
        "training_set = CustomDataset(train_data, tokenizer, MAX_LEN) # Training Set\n",
        "testing_set = CustomDataset(test_data, tokenizer, MAX_LEN)   # Validation Set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wE8FwQL8GBE"
      },
      "source": [
        "# Training and Evaluation Phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ia34HohXuSU"
      },
      "source": [
        "# Device Mapping Select (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.cuda()\n",
        "\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(output_dir=\"./models/model_name\",\n",
        "                                  overwrite_output_dir=True,\n",
        "                                  do_train=True,\n",
        "                                  do_eval=True,\n",
        "                                  per_device_train_batch_size=28,\n",
        "                                  per_device_eval_batch_size=28,\n",
        "                                  num_train_epochs=20,\n",
        "                                  logging_steps=100,\n",
        "                                  logging_first_step=True,\n",
        "                                  save_steps=0,\n",
        "                                  evaluate_during_training=True)\n",
        "\n",
        "# Metric for Performance Evaluation\n",
        "def compute_metrics(p):\n",
        "  preds = np.argmax(p.predictions, axis=1)\n",
        "  return glue_compute_metrics(\"mnli\", preds, p.label_ids)\n",
        "\n",
        "# Trainer for training Model\n",
        "trainer = Trainer(model = model,\n",
        "                  args = training_args,\n",
        "                  train_dataset = training_set,\n",
        "                  eval_dataset = testing_set,\n",
        "                  compute_metrics = compute_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v84hu3rWXzz8"
      },
      "source": [
        "# Training Model\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRnlRPCkZ3-s"
      },
      "source": [
        "# Evaluation of Model on Validation Data\n",
        "trainer.evaluate(testing_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxBJCIwR-1Fx"
      },
      "source": [
        "# Trained Model Save and Load for later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8-c3xRdayNC"
      },
      "source": [
        "# Model Save\n",
        "model_save_path = '/content/drive/My Drive/CONSTRAINT 2021 Projects (AAAI)/Hindi_Task/Weights/XLMR_state_dict_NER_'\n",
        "torch.save(model.state_dict(), model_save_path + str(uuid4()) + '.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjY2SE3nt6o-"
      },
      "source": [
        "# Model Load\n",
        "model_load_path = '/content/drive/My Drive/CONSTRAINT 2021 Projects (AAAI)/Hindi_Task/Weights/XLMR_state_dict_cda2c4d1-e5e0-49a4-acc3-bdf39e74493c.pth'\n",
        "model.load_state_dict(torch.load(model_load_path, map_location=device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9iW-9g--9up"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIegEYr8Ya-0"
      },
      "source": [
        "# Prediction for a given \n",
        "\n",
        "def prepare_features(seq_1, max_seq_length = 128, zero_pad = False, include_CLS_token = True, include_SEP_token = True):\n",
        "    ## Tokenzine Input\n",
        "    tokens_a = tokenizer.tokenize(seq_1)\n",
        "\n",
        "    ## Truncate\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "        tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
        "    ## Initialize Tokens\n",
        "    tokens = []\n",
        "    if include_CLS_token:\n",
        "        tokens.append(tokenizer.cls_token)\n",
        "    ## Add Tokens and separators\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "\n",
        "    if include_SEP_token:\n",
        "        tokens.append(tokenizer.sep_token)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    ## Input Mask \n",
        "    input_mask = [1] * len(input_ids)\n",
        "    ## Zero-pad sequence lenght\n",
        "    if zero_pad:\n",
        "        while len(input_ids) < max_seq_length:\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "    return torch.tensor(input_ids).unsqueeze(0), input_mask\n",
        "\n",
        "def predict(text):\n",
        "  model.eval()\n",
        "  input_feature, _ = prepare_features(text)\n",
        "  if torch.cuda.is_available():\n",
        "    input_feature = input_feature.cuda()\n",
        "  output = model(input_feature)[0]\n",
        "  _, pred_label = torch.max(output.data, 1)\n",
        "  prediction = pred_label\n",
        "  return prediction\n",
        "\n",
        "index = 13                           # Index for identifying post \n",
        "text = test_data['sentence'][index]  # Post as a string\n",
        "\n",
        "pred = predict(text)                 # Prediction of Hostile vs Non-Hostile Class\n",
        "print(pred) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9p7gIEbBnOA"
      },
      "source": [
        "# Prediction Probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4FCS_a1_EUC"
      },
      "source": [
        "def predict_looper(text):\n",
        "  model.eval()\n",
        "  input_feature, _ = prepare_features(text)\n",
        "  if torch.cuda.is_available():\n",
        "    input_feature = input_feature.cuda()\n",
        "  output = model(input_feature)[0]\n",
        "  Softmax = nn.Softmax(dim=1)\n",
        "  prob = Softmax(output)\n",
        "  prob_cpu = prob.cpu().detach().numpy()\n",
        "  return prob_cpu[0]\n",
        "\n",
        "data_1 = train_data\n",
        "data_2 = test_data\n",
        "\n",
        "pred_1 = []\n",
        "for i in range(len(data_1)):\n",
        "  text = data_1['sentence'][i]\n",
        "  pred_1.append(predict_looper(text))\n",
        "\n",
        "pred_2 = []\n",
        "for i in range(len(data_2)):\n",
        "  text = data_2['sentence'][i]\n",
        "  pred_2.append(predict_looper(text))\n",
        "\n",
        "y1 = np.array(pred_1)\n",
        "y2 = np.array(pred_2)\n",
        "\n",
        "np.array(y1).dump(open('Train_Probs_Coarse.npy', 'wb')) # Training Probabilities\n",
        "np.array(y2).dump(open('Test_Probs_Coarse.npy', 'wb'))  # Testing Probabilities"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}