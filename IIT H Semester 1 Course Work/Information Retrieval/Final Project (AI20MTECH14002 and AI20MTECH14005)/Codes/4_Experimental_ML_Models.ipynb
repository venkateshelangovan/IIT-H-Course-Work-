{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Experimental ML Models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_KhSyEzsYS3"
      },
      "source": [
        "# **PROBLEM STATEMENT AND DATA DESCRIPTION:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kE7yG_-sjqy"
      },
      "source": [
        "**Hostile Post Detection in Hindi** :\n",
        "       \n",
        "This subtask focuses on a variety of **hostile posts in Hindi** Devanagari script **collected from Twitter and Facebook**. The set of valid categories are fake news, hate speech, offensive, defamation, and non-hostile posts. It is a **multi-label multi-class classification problem** where each post can belong to one or more of these hostile classes. However, the non-hostile posts cannot be grouped with any other class. The evaluation of this subtask will be two-dimensional as follows:\n",
        "\n",
        "**Coarse-grained evaluation:** It is a binary evaluation of hostile vs non-hostile posts.\n",
        "\n",
        "**Fine-grained evaluation:** It is a fine-grained evaluation of the hostile classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OouLQBpJsm14"
      },
      "source": [
        "**Definitions of the class labels:**\n",
        "\n",
        "**Fake News:** A claim or information that is verified to be not true.\n",
        "\n",
        "**Hate Speech:** A post targeting a specific group of people based on their ethnicity, religious beliefs, geographical belonging, race, etc., with malicious intentions of spreading hate or encouraging violence.\n",
        "\n",
        "**Offensive:** A post containing profanity, impolite, rude, or vulgar language to insult a targeted individual or group.\n",
        "\n",
        "**Defamation:** A mis-information regarding an individual or group.\n",
        "\n",
        "**Non-hostile:** A post without any hostility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9sM5D_tsuoc"
      },
      "source": [
        "**Evaluation Metric:** \n",
        "\n",
        "The official evaluation metric for the shared task is  **weighted-average F1 score**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD5zBSLVtBCM"
      },
      "source": [
        "# **IMPORTING LIBRARIES :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywocFDGny9Xj",
        "outputId": "ffa368c0-bef7-4a24-eddb-52a899b62b06"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZpyjqkEyNmc"
      },
      "source": [
        "# importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math as m \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i_8H7WdtpW5"
      },
      "source": [
        "# **MODELLING:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuPWJpvMtrO-"
      },
      "source": [
        "## **Loading Trained Data returned by mBERT:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGJ1fI19t0Z2"
      },
      "source": [
        "Here the data used for Modelling is the result of cleaned data which is taken from CLF output of multilingual Bert "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arR9JzlSy9Z5"
      },
      "source": [
        "# loading the data returned from Bert\n",
        "train= np.load('/content/gdrive/My Drive/Train_Representations_mBERT.npy',allow_pickle=True)\n",
        "test= np.load('/content/gdrive/My Drive/Test_Representations_mBERT.npy',allow_pickle=True)\n",
        "train_labels= np.load('/content/gdrive/My Drive/Train_Labels.npy',allow_pickle=True)\n",
        "test_labels= np.load('/content/gdrive/My Drive/Test_Labels.npy',allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFESMZeDy9cY"
      },
      "source": [
        "# convert .npy files to dataframe \n",
        "x_train=pd.DataFrame(train)\n",
        "x_test=pd.DataFrame(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "5ma1TIIay9e8",
        "outputId": "0ca89942-5aca-4006-e2c3-4a6bd2255956"
      },
      "source": [
        "x_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>728</th>\n",
              "      <th>729</th>\n",
              "      <th>730</th>\n",
              "      <th>731</th>\n",
              "      <th>732</th>\n",
              "      <th>733</th>\n",
              "      <th>734</th>\n",
              "      <th>735</th>\n",
              "      <th>736</th>\n",
              "      <th>737</th>\n",
              "      <th>738</th>\n",
              "      <th>739</th>\n",
              "      <th>740</th>\n",
              "      <th>741</th>\n",
              "      <th>742</th>\n",
              "      <th>743</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.732588</td>\n",
              "      <td>0.710685</td>\n",
              "      <td>-0.320207</td>\n",
              "      <td>0.830607</td>\n",
              "      <td>0.866882</td>\n",
              "      <td>0.621439</td>\n",
              "      <td>0.289335</td>\n",
              "      <td>-0.811108</td>\n",
              "      <td>-0.455411</td>\n",
              "      <td>-0.793067</td>\n",
              "      <td>-0.815261</td>\n",
              "      <td>-0.893141</td>\n",
              "      <td>0.445581</td>\n",
              "      <td>0.248246</td>\n",
              "      <td>0.089955</td>\n",
              "      <td>-0.693864</td>\n",
              "      <td>-0.589695</td>\n",
              "      <td>-0.619997</td>\n",
              "      <td>0.780396</td>\n",
              "      <td>-0.595645</td>\n",
              "      <td>0.489230</td>\n",
              "      <td>0.353845</td>\n",
              "      <td>-0.676800</td>\n",
              "      <td>-0.829196</td>\n",
              "      <td>-0.450532</td>\n",
              "      <td>-0.168453</td>\n",
              "      <td>-0.457432</td>\n",
              "      <td>-0.902954</td>\n",
              "      <td>-0.067576</td>\n",
              "      <td>0.732085</td>\n",
              "      <td>0.083550</td>\n",
              "      <td>0.639736</td>\n",
              "      <td>-0.757121</td>\n",
              "      <td>-0.876423</td>\n",
              "      <td>0.675147</td>\n",
              "      <td>-0.787643</td>\n",
              "      <td>0.643941</td>\n",
              "      <td>-0.647686</td>\n",
              "      <td>0.529392</td>\n",
              "      <td>-0.057115</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.614511</td>\n",
              "      <td>0.367720</td>\n",
              "      <td>-0.789704</td>\n",
              "      <td>-0.611657</td>\n",
              "      <td>0.876787</td>\n",
              "      <td>-0.732111</td>\n",
              "      <td>0.358988</td>\n",
              "      <td>0.251144</td>\n",
              "      <td>0.214304</td>\n",
              "      <td>0.540675</td>\n",
              "      <td>0.822590</td>\n",
              "      <td>-0.340512</td>\n",
              "      <td>-0.592783</td>\n",
              "      <td>0.825689</td>\n",
              "      <td>-0.306878</td>\n",
              "      <td>0.080668</td>\n",
              "      <td>0.754077</td>\n",
              "      <td>0.546853</td>\n",
              "      <td>-0.784766</td>\n",
              "      <td>-0.717220</td>\n",
              "      <td>-0.476861</td>\n",
              "      <td>0.482289</td>\n",
              "      <td>0.436524</td>\n",
              "      <td>-0.778217</td>\n",
              "      <td>0.866543</td>\n",
              "      <td>0.585288</td>\n",
              "      <td>0.587465</td>\n",
              "      <td>0.987188</td>\n",
              "      <td>-0.673292</td>\n",
              "      <td>-0.672684</td>\n",
              "      <td>0.006438</td>\n",
              "      <td>-0.728774</td>\n",
              "      <td>-0.905429</td>\n",
              "      <td>-0.734196</td>\n",
              "      <td>-0.236235</td>\n",
              "      <td>0.833119</td>\n",
              "      <td>-0.275730</td>\n",
              "      <td>-0.519080</td>\n",
              "      <td>-0.810788</td>\n",
              "      <td>0.524842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.740924</td>\n",
              "      <td>0.030430</td>\n",
              "      <td>-0.449941</td>\n",
              "      <td>0.469014</td>\n",
              "      <td>0.069717</td>\n",
              "      <td>-0.714667</td>\n",
              "      <td>-0.702413</td>\n",
              "      <td>0.031764</td>\n",
              "      <td>-0.694685</td>\n",
              "      <td>0.549749</td>\n",
              "      <td>0.210313</td>\n",
              "      <td>0.810892</td>\n",
              "      <td>-0.790786</td>\n",
              "      <td>-0.743319</td>\n",
              "      <td>-0.381748</td>\n",
              "      <td>-0.178590</td>\n",
              "      <td>0.563576</td>\n",
              "      <td>0.798982</td>\n",
              "      <td>-0.033410</td>\n",
              "      <td>-0.687859</td>\n",
              "      <td>-0.538312</td>\n",
              "      <td>0.626031</td>\n",
              "      <td>0.436732</td>\n",
              "      <td>0.802431</td>\n",
              "      <td>-0.685902</td>\n",
              "      <td>-0.833336</td>\n",
              "      <td>-0.593772</td>\n",
              "      <td>0.730593</td>\n",
              "      <td>0.117065</td>\n",
              "      <td>-0.471525</td>\n",
              "      <td>0.781842</td>\n",
              "      <td>-0.717936</td>\n",
              "      <td>0.165608</td>\n",
              "      <td>0.739031</td>\n",
              "      <td>-0.641787</td>\n",
              "      <td>0.357024</td>\n",
              "      <td>0.812486</td>\n",
              "      <td>-0.319017</td>\n",
              "      <td>0.737088</td>\n",
              "      <td>-0.600888</td>\n",
              "      <td>...</td>\n",
              "      <td>0.167454</td>\n",
              "      <td>0.682751</td>\n",
              "      <td>0.761724</td>\n",
              "      <td>-0.408915</td>\n",
              "      <td>0.329764</td>\n",
              "      <td>-0.688318</td>\n",
              "      <td>-0.222881</td>\n",
              "      <td>0.772413</td>\n",
              "      <td>-0.115886</td>\n",
              "      <td>-0.683200</td>\n",
              "      <td>-0.578816</td>\n",
              "      <td>0.383935</td>\n",
              "      <td>0.750939</td>\n",
              "      <td>-0.655920</td>\n",
              "      <td>-0.723357</td>\n",
              "      <td>0.812904</td>\n",
              "      <td>-0.266620</td>\n",
              "      <td>0.677562</td>\n",
              "      <td>-0.419448</td>\n",
              "      <td>-0.631253</td>\n",
              "      <td>-0.612940</td>\n",
              "      <td>-0.841314</td>\n",
              "      <td>-0.784482</td>\n",
              "      <td>0.846942</td>\n",
              "      <td>-0.781842</td>\n",
              "      <td>-0.749302</td>\n",
              "      <td>-0.143584</td>\n",
              "      <td>0.840547</td>\n",
              "      <td>-0.211017</td>\n",
              "      <td>-0.525201</td>\n",
              "      <td>-0.634581</td>\n",
              "      <td>0.714767</td>\n",
              "      <td>0.662392</td>\n",
              "      <td>-0.015599</td>\n",
              "      <td>-0.742255</td>\n",
              "      <td>-0.734037</td>\n",
              "      <td>0.544445</td>\n",
              "      <td>0.104345</td>\n",
              "      <td>0.148530</td>\n",
              "      <td>0.511468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.781734</td>\n",
              "      <td>-0.003297</td>\n",
              "      <td>-0.471779</td>\n",
              "      <td>0.544349</td>\n",
              "      <td>0.016093</td>\n",
              "      <td>-0.711072</td>\n",
              "      <td>-0.573455</td>\n",
              "      <td>0.095652</td>\n",
              "      <td>-0.770301</td>\n",
              "      <td>0.547369</td>\n",
              "      <td>0.225308</td>\n",
              "      <td>0.751640</td>\n",
              "      <td>-0.771595</td>\n",
              "      <td>-0.774239</td>\n",
              "      <td>-0.541316</td>\n",
              "      <td>-0.166537</td>\n",
              "      <td>0.622713</td>\n",
              "      <td>0.792530</td>\n",
              "      <td>-0.061677</td>\n",
              "      <td>-0.660783</td>\n",
              "      <td>-0.574746</td>\n",
              "      <td>0.671598</td>\n",
              "      <td>0.424552</td>\n",
              "      <td>0.821605</td>\n",
              "      <td>-0.696766</td>\n",
              "      <td>-0.842904</td>\n",
              "      <td>-0.545900</td>\n",
              "      <td>0.766793</td>\n",
              "      <td>0.126307</td>\n",
              "      <td>-0.535921</td>\n",
              "      <td>0.780351</td>\n",
              "      <td>-0.705352</td>\n",
              "      <td>0.243269</td>\n",
              "      <td>0.710903</td>\n",
              "      <td>-0.720899</td>\n",
              "      <td>0.271711</td>\n",
              "      <td>0.835703</td>\n",
              "      <td>-0.303214</td>\n",
              "      <td>0.737156</td>\n",
              "      <td>-0.644042</td>\n",
              "      <td>...</td>\n",
              "      <td>0.266356</td>\n",
              "      <td>0.741960</td>\n",
              "      <td>0.756402</td>\n",
              "      <td>-0.336063</td>\n",
              "      <td>0.470029</td>\n",
              "      <td>-0.687680</td>\n",
              "      <td>-0.324832</td>\n",
              "      <td>0.808871</td>\n",
              "      <td>-0.140541</td>\n",
              "      <td>-0.724489</td>\n",
              "      <td>-0.582541</td>\n",
              "      <td>0.464643</td>\n",
              "      <td>0.749559</td>\n",
              "      <td>-0.621750</td>\n",
              "      <td>-0.727557</td>\n",
              "      <td>0.801081</td>\n",
              "      <td>-0.186788</td>\n",
              "      <td>0.701421</td>\n",
              "      <td>-0.298746</td>\n",
              "      <td>-0.647527</td>\n",
              "      <td>-0.663116</td>\n",
              "      <td>-0.688352</td>\n",
              "      <td>-0.802460</td>\n",
              "      <td>0.845327</td>\n",
              "      <td>-0.801355</td>\n",
              "      <td>-0.748439</td>\n",
              "      <td>-0.244254</td>\n",
              "      <td>0.847448</td>\n",
              "      <td>-0.252710</td>\n",
              "      <td>-0.553241</td>\n",
              "      <td>-0.634092</td>\n",
              "      <td>0.784254</td>\n",
              "      <td>0.610089</td>\n",
              "      <td>-0.050198</td>\n",
              "      <td>-0.763514</td>\n",
              "      <td>-0.744090</td>\n",
              "      <td>0.493762</td>\n",
              "      <td>0.133799</td>\n",
              "      <td>0.105474</td>\n",
              "      <td>0.467554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.744606</td>\n",
              "      <td>0.764933</td>\n",
              "      <td>-0.231929</td>\n",
              "      <td>0.817712</td>\n",
              "      <td>0.833094</td>\n",
              "      <td>0.697169</td>\n",
              "      <td>0.253331</td>\n",
              "      <td>-0.756506</td>\n",
              "      <td>-0.349598</td>\n",
              "      <td>-0.814932</td>\n",
              "      <td>-0.805438</td>\n",
              "      <td>-0.863923</td>\n",
              "      <td>0.531099</td>\n",
              "      <td>0.364355</td>\n",
              "      <td>0.264765</td>\n",
              "      <td>-0.718481</td>\n",
              "      <td>-0.645375</td>\n",
              "      <td>-0.669395</td>\n",
              "      <td>0.812572</td>\n",
              "      <td>-0.542420</td>\n",
              "      <td>0.519840</td>\n",
              "      <td>0.294183</td>\n",
              "      <td>-0.691517</td>\n",
              "      <td>-0.806066</td>\n",
              "      <td>-0.380125</td>\n",
              "      <td>0.099475</td>\n",
              "      <td>-0.388068</td>\n",
              "      <td>-0.907049</td>\n",
              "      <td>-0.049741</td>\n",
              "      <td>0.685456</td>\n",
              "      <td>-0.023177</td>\n",
              "      <td>0.615710</td>\n",
              "      <td>-0.811889</td>\n",
              "      <td>-0.847218</td>\n",
              "      <td>0.635233</td>\n",
              "      <td>-0.817634</td>\n",
              "      <td>0.582290</td>\n",
              "      <td>-0.652200</td>\n",
              "      <td>0.422401</td>\n",
              "      <td>-0.061026</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.658973</td>\n",
              "      <td>0.240340</td>\n",
              "      <td>-0.848518</td>\n",
              "      <td>-0.493888</td>\n",
              "      <td>0.795197</td>\n",
              "      <td>-0.720787</td>\n",
              "      <td>0.348561</td>\n",
              "      <td>-0.032997</td>\n",
              "      <td>0.340780</td>\n",
              "      <td>0.535746</td>\n",
              "      <td>0.838689</td>\n",
              "      <td>-0.365553</td>\n",
              "      <td>-0.637877</td>\n",
              "      <td>0.831474</td>\n",
              "      <td>-0.208785</td>\n",
              "      <td>-0.041729</td>\n",
              "      <td>0.757448</td>\n",
              "      <td>0.448592</td>\n",
              "      <td>-0.752853</td>\n",
              "      <td>-0.630604</td>\n",
              "      <td>-0.483085</td>\n",
              "      <td>0.792773</td>\n",
              "      <td>0.544032</td>\n",
              "      <td>-0.732893</td>\n",
              "      <td>0.858338</td>\n",
              "      <td>0.608440</td>\n",
              "      <td>0.596441</td>\n",
              "      <td>0.969823</td>\n",
              "      <td>-0.650090</td>\n",
              "      <td>-0.692505</td>\n",
              "      <td>0.113471</td>\n",
              "      <td>-0.746093</td>\n",
              "      <td>-0.880959</td>\n",
              "      <td>-0.672832</td>\n",
              "      <td>-0.214177</td>\n",
              "      <td>0.877333</td>\n",
              "      <td>-0.356047</td>\n",
              "      <td>-0.510230</td>\n",
              "      <td>-0.762685</td>\n",
              "      <td>0.506918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.774036</td>\n",
              "      <td>0.016017</td>\n",
              "      <td>-0.451547</td>\n",
              "      <td>0.551840</td>\n",
              "      <td>0.039809</td>\n",
              "      <td>-0.728371</td>\n",
              "      <td>-0.599640</td>\n",
              "      <td>0.084241</td>\n",
              "      <td>-0.744135</td>\n",
              "      <td>0.554946</td>\n",
              "      <td>0.202289</td>\n",
              "      <td>0.765284</td>\n",
              "      <td>-0.775677</td>\n",
              "      <td>-0.772673</td>\n",
              "      <td>-0.511907</td>\n",
              "      <td>-0.155191</td>\n",
              "      <td>0.643631</td>\n",
              "      <td>0.773615</td>\n",
              "      <td>-0.096573</td>\n",
              "      <td>-0.671992</td>\n",
              "      <td>-0.560466</td>\n",
              "      <td>0.689830</td>\n",
              "      <td>0.394631</td>\n",
              "      <td>0.794537</td>\n",
              "      <td>-0.721360</td>\n",
              "      <td>-0.827626</td>\n",
              "      <td>-0.516831</td>\n",
              "      <td>0.745294</td>\n",
              "      <td>0.149893</td>\n",
              "      <td>-0.535676</td>\n",
              "      <td>0.797109</td>\n",
              "      <td>-0.721649</td>\n",
              "      <td>0.191339</td>\n",
              "      <td>0.727591</td>\n",
              "      <td>-0.731698</td>\n",
              "      <td>0.238135</td>\n",
              "      <td>0.851838</td>\n",
              "      <td>-0.333948</td>\n",
              "      <td>0.727532</td>\n",
              "      <td>-0.640501</td>\n",
              "      <td>...</td>\n",
              "      <td>0.280129</td>\n",
              "      <td>0.719920</td>\n",
              "      <td>0.744078</td>\n",
              "      <td>-0.340250</td>\n",
              "      <td>0.457225</td>\n",
              "      <td>-0.698597</td>\n",
              "      <td>-0.300760</td>\n",
              "      <td>0.780165</td>\n",
              "      <td>-0.115395</td>\n",
              "      <td>-0.700699</td>\n",
              "      <td>-0.567758</td>\n",
              "      <td>0.461888</td>\n",
              "      <td>0.762728</td>\n",
              "      <td>-0.631608</td>\n",
              "      <td>-0.742902</td>\n",
              "      <td>0.791906</td>\n",
              "      <td>-0.211219</td>\n",
              "      <td>0.718073</td>\n",
              "      <td>-0.336529</td>\n",
              "      <td>-0.648245</td>\n",
              "      <td>-0.657412</td>\n",
              "      <td>-0.666318</td>\n",
              "      <td>-0.790687</td>\n",
              "      <td>0.862514</td>\n",
              "      <td>-0.780496</td>\n",
              "      <td>-0.749193</td>\n",
              "      <td>-0.194627</td>\n",
              "      <td>0.824975</td>\n",
              "      <td>-0.248652</td>\n",
              "      <td>-0.545651</td>\n",
              "      <td>-0.619510</td>\n",
              "      <td>0.768704</td>\n",
              "      <td>0.643254</td>\n",
              "      <td>-0.025785</td>\n",
              "      <td>-0.790703</td>\n",
              "      <td>-0.756496</td>\n",
              "      <td>0.534121</td>\n",
              "      <td>0.100024</td>\n",
              "      <td>0.104016</td>\n",
              "      <td>0.476376</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 768 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       765       766       767\n",
              "0 -0.732588  0.710685 -0.320207  ... -0.519080 -0.810788  0.524842\n",
              "1  0.740924  0.030430 -0.449941  ...  0.104345  0.148530  0.511468\n",
              "2  0.781734 -0.003297 -0.471779  ...  0.133799  0.105474  0.467554\n",
              "3 -0.744606  0.764933 -0.231929  ... -0.510230 -0.762685  0.506918\n",
              "4  0.774036  0.016017 -0.451547  ...  0.100024  0.104016  0.476376\n",
              "\n",
              "[5 rows x 768 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "Hk0Yn4hny9hR",
        "outputId": "9d7d3015-db74-4e77-ee83-79e87628b9d6"
      },
      "source": [
        "x_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>728</th>\n",
              "      <th>729</th>\n",
              "      <th>730</th>\n",
              "      <th>731</th>\n",
              "      <th>732</th>\n",
              "      <th>733</th>\n",
              "      <th>734</th>\n",
              "      <th>735</th>\n",
              "      <th>736</th>\n",
              "      <th>737</th>\n",
              "      <th>738</th>\n",
              "      <th>739</th>\n",
              "      <th>740</th>\n",
              "      <th>741</th>\n",
              "      <th>742</th>\n",
              "      <th>743</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.727502</td>\n",
              "      <td>0.011243</td>\n",
              "      <td>-0.467984</td>\n",
              "      <td>0.571245</td>\n",
              "      <td>0.060416</td>\n",
              "      <td>-0.726072</td>\n",
              "      <td>-0.649169</td>\n",
              "      <td>0.146978</td>\n",
              "      <td>-0.746521</td>\n",
              "      <td>0.531886</td>\n",
              "      <td>0.211846</td>\n",
              "      <td>0.770344</td>\n",
              "      <td>-0.761274</td>\n",
              "      <td>-0.756878</td>\n",
              "      <td>-0.541705</td>\n",
              "      <td>-0.131372</td>\n",
              "      <td>0.573518</td>\n",
              "      <td>0.776543</td>\n",
              "      <td>-0.105236</td>\n",
              "      <td>-0.687468</td>\n",
              "      <td>-0.585769</td>\n",
              "      <td>0.681613</td>\n",
              "      <td>0.428735</td>\n",
              "      <td>0.815939</td>\n",
              "      <td>-0.701426</td>\n",
              "      <td>-0.837903</td>\n",
              "      <td>-0.573501</td>\n",
              "      <td>0.748696</td>\n",
              "      <td>0.100726</td>\n",
              "      <td>-0.563301</td>\n",
              "      <td>0.786285</td>\n",
              "      <td>-0.715220</td>\n",
              "      <td>0.172105</td>\n",
              "      <td>0.750478</td>\n",
              "      <td>-0.706496</td>\n",
              "      <td>0.309624</td>\n",
              "      <td>0.841288</td>\n",
              "      <td>-0.244592</td>\n",
              "      <td>0.747568</td>\n",
              "      <td>-0.646657</td>\n",
              "      <td>...</td>\n",
              "      <td>0.210935</td>\n",
              "      <td>0.729748</td>\n",
              "      <td>0.757902</td>\n",
              "      <td>-0.312272</td>\n",
              "      <td>0.388349</td>\n",
              "      <td>-0.678788</td>\n",
              "      <td>-0.299276</td>\n",
              "      <td>0.769832</td>\n",
              "      <td>-0.114645</td>\n",
              "      <td>-0.749215</td>\n",
              "      <td>-0.580068</td>\n",
              "      <td>0.470850</td>\n",
              "      <td>0.754006</td>\n",
              "      <td>-0.615156</td>\n",
              "      <td>-0.733181</td>\n",
              "      <td>0.802749</td>\n",
              "      <td>-0.168567</td>\n",
              "      <td>0.698226</td>\n",
              "      <td>-0.321995</td>\n",
              "      <td>-0.674512</td>\n",
              "      <td>-0.619030</td>\n",
              "      <td>-0.701229</td>\n",
              "      <td>-0.792387</td>\n",
              "      <td>0.852660</td>\n",
              "      <td>-0.797343</td>\n",
              "      <td>-0.743494</td>\n",
              "      <td>-0.162317</td>\n",
              "      <td>0.797574</td>\n",
              "      <td>-0.263969</td>\n",
              "      <td>-0.547296</td>\n",
              "      <td>-0.612569</td>\n",
              "      <td>0.751568</td>\n",
              "      <td>0.654832</td>\n",
              "      <td>0.047102</td>\n",
              "      <td>-0.781962</td>\n",
              "      <td>-0.757271</td>\n",
              "      <td>0.542628</td>\n",
              "      <td>0.139290</td>\n",
              "      <td>0.104002</td>\n",
              "      <td>0.504401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.676964</td>\n",
              "      <td>0.685156</td>\n",
              "      <td>-0.384752</td>\n",
              "      <td>0.869102</td>\n",
              "      <td>0.862156</td>\n",
              "      <td>0.618207</td>\n",
              "      <td>0.221776</td>\n",
              "      <td>-0.798615</td>\n",
              "      <td>-0.434710</td>\n",
              "      <td>-0.812352</td>\n",
              "      <td>-0.796630</td>\n",
              "      <td>-0.904728</td>\n",
              "      <td>0.407483</td>\n",
              "      <td>0.202314</td>\n",
              "      <td>0.161006</td>\n",
              "      <td>-0.718164</td>\n",
              "      <td>-0.450518</td>\n",
              "      <td>-0.619480</td>\n",
              "      <td>0.815309</td>\n",
              "      <td>-0.618710</td>\n",
              "      <td>0.466710</td>\n",
              "      <td>0.325960</td>\n",
              "      <td>-0.698508</td>\n",
              "      <td>-0.805896</td>\n",
              "      <td>-0.469325</td>\n",
              "      <td>-0.154987</td>\n",
              "      <td>-0.375674</td>\n",
              "      <td>-0.891725</td>\n",
              "      <td>0.052837</td>\n",
              "      <td>0.740693</td>\n",
              "      <td>0.165952</td>\n",
              "      <td>0.644648</td>\n",
              "      <td>-0.746170</td>\n",
              "      <td>-0.847375</td>\n",
              "      <td>0.671502</td>\n",
              "      <td>-0.817470</td>\n",
              "      <td>0.660787</td>\n",
              "      <td>-0.723469</td>\n",
              "      <td>0.486908</td>\n",
              "      <td>-0.023316</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.609398</td>\n",
              "      <td>0.495201</td>\n",
              "      <td>-0.816249</td>\n",
              "      <td>-0.618296</td>\n",
              "      <td>0.921647</td>\n",
              "      <td>-0.803137</td>\n",
              "      <td>0.322692</td>\n",
              "      <td>0.151149</td>\n",
              "      <td>0.293551</td>\n",
              "      <td>0.611904</td>\n",
              "      <td>0.811154</td>\n",
              "      <td>-0.447241</td>\n",
              "      <td>-0.620624</td>\n",
              "      <td>0.793466</td>\n",
              "      <td>-0.311524</td>\n",
              "      <td>0.013621</td>\n",
              "      <td>0.687390</td>\n",
              "      <td>0.574774</td>\n",
              "      <td>-0.758605</td>\n",
              "      <td>-0.685408</td>\n",
              "      <td>-0.551354</td>\n",
              "      <td>0.655503</td>\n",
              "      <td>0.361233</td>\n",
              "      <td>-0.715680</td>\n",
              "      <td>0.891313</td>\n",
              "      <td>0.545944</td>\n",
              "      <td>0.629721</td>\n",
              "      <td>0.994584</td>\n",
              "      <td>-0.621504</td>\n",
              "      <td>-0.794502</td>\n",
              "      <td>0.022901</td>\n",
              "      <td>-0.710130</td>\n",
              "      <td>-0.898802</td>\n",
              "      <td>-0.731665</td>\n",
              "      <td>-0.354049</td>\n",
              "      <td>0.865085</td>\n",
              "      <td>-0.323622</td>\n",
              "      <td>-0.545879</td>\n",
              "      <td>-0.814437</td>\n",
              "      <td>0.535207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.777822</td>\n",
              "      <td>0.004835</td>\n",
              "      <td>-0.448092</td>\n",
              "      <td>0.573206</td>\n",
              "      <td>0.021950</td>\n",
              "      <td>-0.705602</td>\n",
              "      <td>-0.582029</td>\n",
              "      <td>0.052576</td>\n",
              "      <td>-0.775060</td>\n",
              "      <td>0.569575</td>\n",
              "      <td>0.180514</td>\n",
              "      <td>0.742350</td>\n",
              "      <td>-0.756073</td>\n",
              "      <td>-0.762054</td>\n",
              "      <td>-0.528167</td>\n",
              "      <td>-0.151489</td>\n",
              "      <td>0.639000</td>\n",
              "      <td>0.795091</td>\n",
              "      <td>-0.031133</td>\n",
              "      <td>-0.622948</td>\n",
              "      <td>-0.601213</td>\n",
              "      <td>0.665174</td>\n",
              "      <td>0.435739</td>\n",
              "      <td>0.819363</td>\n",
              "      <td>-0.708881</td>\n",
              "      <td>-0.841029</td>\n",
              "      <td>-0.556917</td>\n",
              "      <td>0.773052</td>\n",
              "      <td>0.161851</td>\n",
              "      <td>-0.548974</td>\n",
              "      <td>0.795311</td>\n",
              "      <td>-0.708155</td>\n",
              "      <td>0.230134</td>\n",
              "      <td>0.723631</td>\n",
              "      <td>-0.718151</td>\n",
              "      <td>0.310568</td>\n",
              "      <td>0.824735</td>\n",
              "      <td>-0.252165</td>\n",
              "      <td>0.734213</td>\n",
              "      <td>-0.649410</td>\n",
              "      <td>...</td>\n",
              "      <td>0.296795</td>\n",
              "      <td>0.731696</td>\n",
              "      <td>0.760132</td>\n",
              "      <td>-0.364206</td>\n",
              "      <td>0.496273</td>\n",
              "      <td>-0.697339</td>\n",
              "      <td>-0.346969</td>\n",
              "      <td>0.790176</td>\n",
              "      <td>-0.101821</td>\n",
              "      <td>-0.707041</td>\n",
              "      <td>-0.600261</td>\n",
              "      <td>0.460348</td>\n",
              "      <td>0.758464</td>\n",
              "      <td>-0.632417</td>\n",
              "      <td>-0.755153</td>\n",
              "      <td>0.797012</td>\n",
              "      <td>-0.146669</td>\n",
              "      <td>0.680862</td>\n",
              "      <td>-0.324084</td>\n",
              "      <td>-0.634997</td>\n",
              "      <td>-0.627225</td>\n",
              "      <td>-0.699401</td>\n",
              "      <td>-0.801236</td>\n",
              "      <td>0.844131</td>\n",
              "      <td>-0.793721</td>\n",
              "      <td>-0.746827</td>\n",
              "      <td>-0.216187</td>\n",
              "      <td>0.847595</td>\n",
              "      <td>-0.237989</td>\n",
              "      <td>-0.538315</td>\n",
              "      <td>-0.620506</td>\n",
              "      <td>0.772085</td>\n",
              "      <td>0.625093</td>\n",
              "      <td>-0.077928</td>\n",
              "      <td>-0.770437</td>\n",
              "      <td>-0.727151</td>\n",
              "      <td>0.525415</td>\n",
              "      <td>0.191602</td>\n",
              "      <td>0.096546</td>\n",
              "      <td>0.467808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.646729</td>\n",
              "      <td>0.311024</td>\n",
              "      <td>-0.582803</td>\n",
              "      <td>0.718245</td>\n",
              "      <td>0.371884</td>\n",
              "      <td>-0.663043</td>\n",
              "      <td>-0.613093</td>\n",
              "      <td>-0.268285</td>\n",
              "      <td>-0.752557</td>\n",
              "      <td>0.401028</td>\n",
              "      <td>-0.075087</td>\n",
              "      <td>0.699511</td>\n",
              "      <td>-0.777632</td>\n",
              "      <td>-0.682590</td>\n",
              "      <td>-0.511369</td>\n",
              "      <td>-0.528752</td>\n",
              "      <td>0.448617</td>\n",
              "      <td>0.628861</td>\n",
              "      <td>0.151190</td>\n",
              "      <td>-0.651223</td>\n",
              "      <td>-0.548580</td>\n",
              "      <td>0.739821</td>\n",
              "      <td>0.207785</td>\n",
              "      <td>0.749402</td>\n",
              "      <td>-0.774966</td>\n",
              "      <td>-0.822134</td>\n",
              "      <td>-0.632910</td>\n",
              "      <td>0.464108</td>\n",
              "      <td>0.010848</td>\n",
              "      <td>-0.267734</td>\n",
              "      <td>0.805370</td>\n",
              "      <td>-0.649244</td>\n",
              "      <td>-0.101725</td>\n",
              "      <td>0.619055</td>\n",
              "      <td>-0.614371</td>\n",
              "      <td>0.002020</td>\n",
              "      <td>0.886996</td>\n",
              "      <td>-0.613559</td>\n",
              "      <td>0.681640</td>\n",
              "      <td>-0.718174</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.159081</td>\n",
              "      <td>0.799277</td>\n",
              "      <td>0.595288</td>\n",
              "      <td>-0.427722</td>\n",
              "      <td>0.267177</td>\n",
              "      <td>-0.818863</td>\n",
              "      <td>-0.107695</td>\n",
              "      <td>0.819174</td>\n",
              "      <td>0.187874</td>\n",
              "      <td>-0.737861</td>\n",
              "      <td>-0.344020</td>\n",
              "      <td>0.318540</td>\n",
              "      <td>0.663859</td>\n",
              "      <td>-0.554339</td>\n",
              "      <td>-0.768714</td>\n",
              "      <td>0.730115</td>\n",
              "      <td>-0.005627</td>\n",
              "      <td>0.738927</td>\n",
              "      <td>-0.367838</td>\n",
              "      <td>-0.660598</td>\n",
              "      <td>-0.748839</td>\n",
              "      <td>-0.533240</td>\n",
              "      <td>-0.822131</td>\n",
              "      <td>0.786770</td>\n",
              "      <td>-0.734505</td>\n",
              "      <td>-0.706263</td>\n",
              "      <td>0.005604</td>\n",
              "      <td>0.840597</td>\n",
              "      <td>-0.432220</td>\n",
              "      <td>-0.673963</td>\n",
              "      <td>-0.625916</td>\n",
              "      <td>0.683952</td>\n",
              "      <td>0.553784</td>\n",
              "      <td>-0.186486</td>\n",
              "      <td>-0.820705</td>\n",
              "      <td>-0.601467</td>\n",
              "      <td>0.480933</td>\n",
              "      <td>0.082433</td>\n",
              "      <td>-0.142637</td>\n",
              "      <td>0.603286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.743800</td>\n",
              "      <td>0.060436</td>\n",
              "      <td>-0.539539</td>\n",
              "      <td>0.605774</td>\n",
              "      <td>0.094064</td>\n",
              "      <td>-0.725081</td>\n",
              "      <td>-0.540902</td>\n",
              "      <td>0.090068</td>\n",
              "      <td>-0.768050</td>\n",
              "      <td>0.531974</td>\n",
              "      <td>0.177663</td>\n",
              "      <td>0.747387</td>\n",
              "      <td>-0.762427</td>\n",
              "      <td>-0.769673</td>\n",
              "      <td>-0.554799</td>\n",
              "      <td>-0.252582</td>\n",
              "      <td>0.609113</td>\n",
              "      <td>0.750427</td>\n",
              "      <td>-0.051112</td>\n",
              "      <td>-0.640613</td>\n",
              "      <td>-0.562181</td>\n",
              "      <td>0.693705</td>\n",
              "      <td>0.401013</td>\n",
              "      <td>0.851980</td>\n",
              "      <td>-0.712351</td>\n",
              "      <td>-0.819551</td>\n",
              "      <td>-0.497157</td>\n",
              "      <td>0.691706</td>\n",
              "      <td>0.236964</td>\n",
              "      <td>-0.529363</td>\n",
              "      <td>0.799092</td>\n",
              "      <td>-0.694873</td>\n",
              "      <td>0.133512</td>\n",
              "      <td>0.732512</td>\n",
              "      <td>-0.708701</td>\n",
              "      <td>0.244365</td>\n",
              "      <td>0.843869</td>\n",
              "      <td>-0.359573</td>\n",
              "      <td>0.713257</td>\n",
              "      <td>-0.704495</td>\n",
              "      <td>...</td>\n",
              "      <td>0.164749</td>\n",
              "      <td>0.783881</td>\n",
              "      <td>0.750262</td>\n",
              "      <td>-0.407089</td>\n",
              "      <td>0.263336</td>\n",
              "      <td>-0.692100</td>\n",
              "      <td>-0.299789</td>\n",
              "      <td>0.802011</td>\n",
              "      <td>0.018707</td>\n",
              "      <td>-0.739840</td>\n",
              "      <td>-0.556438</td>\n",
              "      <td>0.410863</td>\n",
              "      <td>0.726612</td>\n",
              "      <td>-0.611489</td>\n",
              "      <td>-0.771832</td>\n",
              "      <td>0.769622</td>\n",
              "      <td>-0.070092</td>\n",
              "      <td>0.706637</td>\n",
              "      <td>-0.235830</td>\n",
              "      <td>-0.675641</td>\n",
              "      <td>-0.711078</td>\n",
              "      <td>-0.675566</td>\n",
              "      <td>-0.800038</td>\n",
              "      <td>0.844119</td>\n",
              "      <td>-0.799347</td>\n",
              "      <td>-0.752692</td>\n",
              "      <td>-0.109745</td>\n",
              "      <td>0.713799</td>\n",
              "      <td>-0.364701</td>\n",
              "      <td>-0.559497</td>\n",
              "      <td>-0.613820</td>\n",
              "      <td>0.742403</td>\n",
              "      <td>0.651035</td>\n",
              "      <td>-0.125933</td>\n",
              "      <td>-0.811147</td>\n",
              "      <td>-0.750282</td>\n",
              "      <td>0.483053</td>\n",
              "      <td>0.171151</td>\n",
              "      <td>0.030553</td>\n",
              "      <td>0.495641</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 768 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       765       766       767\n",
              "0  0.727502  0.011243 -0.467984  ...  0.139290  0.104002  0.504401\n",
              "1 -0.676964  0.685156 -0.384752  ... -0.545879 -0.814437  0.535207\n",
              "2  0.777822  0.004835 -0.448092  ...  0.191602  0.096546  0.467808\n",
              "3  0.646729  0.311024 -0.582803  ...  0.082433 -0.142637  0.603286\n",
              "4  0.743800  0.060436 -0.539539  ...  0.171151  0.030553  0.495641\n",
              "\n",
              "[5 rows x 768 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCxdoP09y9j-",
        "outputId": "fa0abda2-d209-47a9-902a-ce101385165f"
      },
      "source": [
        "print(\"x_train shape \",x_train.shape)\n",
        "print(\"x_test shape \",x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape  (5728, 768)\n",
            "x_test shape  (811, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FnGwSngy9mb"
      },
      "source": [
        "y_train=pd.DataFrame(train_labels)\n",
        "y_test=pd.DataFrame(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "ijRyIp_by9pJ",
        "outputId": "95efa615-efe6-43fb-8650-4c9dda80b56d"
      },
      "source": [
        "x_train.corr().head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>728</th>\n",
              "      <th>729</th>\n",
              "      <th>730</th>\n",
              "      <th>731</th>\n",
              "      <th>732</th>\n",
              "      <th>733</th>\n",
              "      <th>734</th>\n",
              "      <th>735</th>\n",
              "      <th>736</th>\n",
              "      <th>737</th>\n",
              "      <th>738</th>\n",
              "      <th>739</th>\n",
              "      <th>740</th>\n",
              "      <th>741</th>\n",
              "      <th>742</th>\n",
              "      <th>743</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.993814</td>\n",
              "      <td>-0.872804</td>\n",
              "      <td>-0.977676</td>\n",
              "      <td>-0.995235</td>\n",
              "      <td>-0.997506</td>\n",
              "      <td>-0.994642</td>\n",
              "      <td>0.993961</td>\n",
              "      <td>-0.958390</td>\n",
              "      <td>0.998938</td>\n",
              "      <td>0.997769</td>\n",
              "      <td>0.998833</td>\n",
              "      <td>-0.996133</td>\n",
              "      <td>-0.988269</td>\n",
              "      <td>-0.983723</td>\n",
              "      <td>0.989714</td>\n",
              "      <td>0.997675</td>\n",
              "      <td>0.998130</td>\n",
              "      <td>-0.996679</td>\n",
              "      <td>-0.691830</td>\n",
              "      <td>-0.997037</td>\n",
              "      <td>0.977998</td>\n",
              "      <td>0.998192</td>\n",
              "      <td>0.999046</td>\n",
              "      <td>-0.970912</td>\n",
              "      <td>-0.979034</td>\n",
              "      <td>-0.808424</td>\n",
              "      <td>0.999217</td>\n",
              "      <td>0.530217</td>\n",
              "      <td>-0.998230</td>\n",
              "      <td>0.987804</td>\n",
              "      <td>-0.998687</td>\n",
              "      <td>0.997253</td>\n",
              "      <td>0.999015</td>\n",
              "      <td>-0.998987</td>\n",
              "      <td>0.997660</td>\n",
              "      <td>0.962158</td>\n",
              "      <td>0.973958</td>\n",
              "      <td>0.951700</td>\n",
              "      <td>-0.992519</td>\n",
              "      <td>...</td>\n",
              "      <td>0.995242</td>\n",
              "      <td>0.942912</td>\n",
              "      <td>0.999042</td>\n",
              "      <td>0.939332</td>\n",
              "      <td>-0.950051</td>\n",
              "      <td>0.532707</td>\n",
              "      <td>-0.986163</td>\n",
              "      <td>0.971134</td>\n",
              "      <td>-0.963083</td>\n",
              "      <td>-0.995155</td>\n",
              "      <td>-0.999120</td>\n",
              "      <td>0.995344</td>\n",
              "      <td>0.998959</td>\n",
              "      <td>-0.999279</td>\n",
              "      <td>-0.976301</td>\n",
              "      <td>0.986288</td>\n",
              "      <td>-0.996872</td>\n",
              "      <td>0.942048</td>\n",
              "      <td>0.991592</td>\n",
              "      <td>0.438315</td>\n",
              "      <td>-0.935837</td>\n",
              "      <td>-0.986969</td>\n",
              "      <td>-0.994885</td>\n",
              "      <td>0.999231</td>\n",
              "      <td>-0.999084</td>\n",
              "      <td>-0.998618</td>\n",
              "      <td>-0.995724</td>\n",
              "      <td>-0.915534</td>\n",
              "      <td>0.964259</td>\n",
              "      <td>0.942236</td>\n",
              "      <td>-0.985010</td>\n",
              "      <td>0.998586</td>\n",
              "      <td>0.998825</td>\n",
              "      <td>0.987768</td>\n",
              "      <td>-0.985537</td>\n",
              "      <td>-0.999005</td>\n",
              "      <td>0.996065</td>\n",
              "      <td>0.992500</td>\n",
              "      <td>0.995524</td>\n",
              "      <td>0.043075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.993814</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.857110</td>\n",
              "      <td>0.974137</td>\n",
              "      <td>0.992913</td>\n",
              "      <td>0.991827</td>\n",
              "      <td>0.984496</td>\n",
              "      <td>-0.989295</td>\n",
              "      <td>0.953135</td>\n",
              "      <td>-0.993478</td>\n",
              "      <td>-0.994343</td>\n",
              "      <td>-0.992178</td>\n",
              "      <td>0.988989</td>\n",
              "      <td>0.981090</td>\n",
              "      <td>0.976204</td>\n",
              "      <td>-0.989813</td>\n",
              "      <td>-0.992981</td>\n",
              "      <td>-0.990798</td>\n",
              "      <td>0.992206</td>\n",
              "      <td>0.690540</td>\n",
              "      <td>0.992039</td>\n",
              "      <td>-0.963727</td>\n",
              "      <td>-0.993931</td>\n",
              "      <td>-0.992650</td>\n",
              "      <td>0.963281</td>\n",
              "      <td>0.970991</td>\n",
              "      <td>0.793018</td>\n",
              "      <td>-0.995194</td>\n",
              "      <td>-0.521272</td>\n",
              "      <td>0.992765</td>\n",
              "      <td>-0.977434</td>\n",
              "      <td>0.992343</td>\n",
              "      <td>-0.994691</td>\n",
              "      <td>-0.993192</td>\n",
              "      <td>0.992851</td>\n",
              "      <td>-0.991076</td>\n",
              "      <td>-0.948024</td>\n",
              "      <td>-0.979705</td>\n",
              "      <td>-0.942985</td>\n",
              "      <td>0.981967</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.995178</td>\n",
              "      <td>-0.935873</td>\n",
              "      <td>-0.993714</td>\n",
              "      <td>-0.937531</td>\n",
              "      <td>0.924363</td>\n",
              "      <td>-0.562996</td>\n",
              "      <td>0.982048</td>\n",
              "      <td>-0.963193</td>\n",
              "      <td>0.965383</td>\n",
              "      <td>0.988522</td>\n",
              "      <td>0.993712</td>\n",
              "      <td>-0.989851</td>\n",
              "      <td>-0.992158</td>\n",
              "      <td>0.993222</td>\n",
              "      <td>0.966279</td>\n",
              "      <td>-0.977746</td>\n",
              "      <td>0.991892</td>\n",
              "      <td>-0.934728</td>\n",
              "      <td>-0.986926</td>\n",
              "      <td>-0.441396</td>\n",
              "      <td>0.932846</td>\n",
              "      <td>0.983485</td>\n",
              "      <td>0.988617</td>\n",
              "      <td>-0.992637</td>\n",
              "      <td>0.993078</td>\n",
              "      <td>0.991675</td>\n",
              "      <td>0.992929</td>\n",
              "      <td>0.887850</td>\n",
              "      <td>-0.967075</td>\n",
              "      <td>-0.938587</td>\n",
              "      <td>0.977145</td>\n",
              "      <td>-0.993442</td>\n",
              "      <td>-0.992301</td>\n",
              "      <td>-0.984232</td>\n",
              "      <td>0.973762</td>\n",
              "      <td>0.993066</td>\n",
              "      <td>-0.989859</td>\n",
              "      <td>-0.990122</td>\n",
              "      <td>-0.990480</td>\n",
              "      <td>-0.010351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.872804</td>\n",
              "      <td>0.857110</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.817612</td>\n",
              "      <td>0.856579</td>\n",
              "      <td>0.885273</td>\n",
              "      <td>0.870050</td>\n",
              "      <td>-0.859857</td>\n",
              "      <td>0.911200</td>\n",
              "      <td>-0.868552</td>\n",
              "      <td>-0.867244</td>\n",
              "      <td>-0.870379</td>\n",
              "      <td>0.896775</td>\n",
              "      <td>0.902293</td>\n",
              "      <td>0.900134</td>\n",
              "      <td>-0.835110</td>\n",
              "      <td>-0.879012</td>\n",
              "      <td>-0.883731</td>\n",
              "      <td>0.876278</td>\n",
              "      <td>0.781998</td>\n",
              "      <td>0.884589</td>\n",
              "      <td>-0.907133</td>\n",
              "      <td>-0.866591</td>\n",
              "      <td>-0.874966</td>\n",
              "      <td>0.903048</td>\n",
              "      <td>0.911988</td>\n",
              "      <td>0.809676</td>\n",
              "      <td>-0.869448</td>\n",
              "      <td>-0.382772</td>\n",
              "      <td>0.869370</td>\n",
              "      <td>-0.898211</td>\n",
              "      <td>0.876121</td>\n",
              "      <td>-0.872596</td>\n",
              "      <td>-0.870580</td>\n",
              "      <td>0.874185</td>\n",
              "      <td>-0.868319</td>\n",
              "      <td>-0.923824</td>\n",
              "      <td>-0.813103</td>\n",
              "      <td>-0.897884</td>\n",
              "      <td>0.893496</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.860244</td>\n",
              "      <td>-0.915175</td>\n",
              "      <td>-0.872665</td>\n",
              "      <td>-0.773484</td>\n",
              "      <td>0.830587</td>\n",
              "      <td>-0.314754</td>\n",
              "      <td>0.874986</td>\n",
              "      <td>-0.913656</td>\n",
              "      <td>0.871441</td>\n",
              "      <td>0.879794</td>\n",
              "      <td>0.869299</td>\n",
              "      <td>-0.867181</td>\n",
              "      <td>-0.878515</td>\n",
              "      <td>0.872270</td>\n",
              "      <td>0.901761</td>\n",
              "      <td>-0.900900</td>\n",
              "      <td>0.870089</td>\n",
              "      <td>-0.904363</td>\n",
              "      <td>-0.865325</td>\n",
              "      <td>-0.206911</td>\n",
              "      <td>0.875823</td>\n",
              "      <td>0.868541</td>\n",
              "      <td>0.893090</td>\n",
              "      <td>-0.873188</td>\n",
              "      <td>0.870684</td>\n",
              "      <td>0.884812</td>\n",
              "      <td>0.863474</td>\n",
              "      <td>0.831507</td>\n",
              "      <td>-0.790235</td>\n",
              "      <td>-0.754868</td>\n",
              "      <td>0.917224</td>\n",
              "      <td>-0.877312</td>\n",
              "      <td>-0.870673</td>\n",
              "      <td>-0.851772</td>\n",
              "      <td>0.913787</td>\n",
              "      <td>0.876317</td>\n",
              "      <td>-0.875914</td>\n",
              "      <td>-0.868185</td>\n",
              "      <td>-0.852478</td>\n",
              "      <td>-0.168928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.977676</td>\n",
              "      <td>0.974137</td>\n",
              "      <td>0.817612</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.982416</td>\n",
              "      <td>0.975620</td>\n",
              "      <td>0.980060</td>\n",
              "      <td>-0.983200</td>\n",
              "      <td>0.905755</td>\n",
              "      <td>-0.978770</td>\n",
              "      <td>-0.982078</td>\n",
              "      <td>-0.981668</td>\n",
              "      <td>0.971724</td>\n",
              "      <td>0.950168</td>\n",
              "      <td>0.940215</td>\n",
              "      <td>-0.974682</td>\n",
              "      <td>-0.971048</td>\n",
              "      <td>-0.971854</td>\n",
              "      <td>0.980595</td>\n",
              "      <td>0.651096</td>\n",
              "      <td>0.968204</td>\n",
              "      <td>-0.945812</td>\n",
              "      <td>-0.977620</td>\n",
              "      <td>-0.978934</td>\n",
              "      <td>0.945053</td>\n",
              "      <td>0.935938</td>\n",
              "      <td>0.790413</td>\n",
              "      <td>-0.979563</td>\n",
              "      <td>-0.462475</td>\n",
              "      <td>0.981731</td>\n",
              "      <td>-0.949175</td>\n",
              "      <td>0.980266</td>\n",
              "      <td>-0.975350</td>\n",
              "      <td>-0.980672</td>\n",
              "      <td>0.979321</td>\n",
              "      <td>-0.978982</td>\n",
              "      <td>-0.923453</td>\n",
              "      <td>-0.960524</td>\n",
              "      <td>-0.904829</td>\n",
              "      <td>0.967047</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.974612</td>\n",
              "      <td>-0.878380</td>\n",
              "      <td>-0.978442</td>\n",
              "      <td>-0.960850</td>\n",
              "      <td>0.944819</td>\n",
              "      <td>-0.620411</td>\n",
              "      <td>0.950358</td>\n",
              "      <td>-0.925990</td>\n",
              "      <td>0.928379</td>\n",
              "      <td>0.976326</td>\n",
              "      <td>0.979854</td>\n",
              "      <td>-0.980922</td>\n",
              "      <td>-0.974644</td>\n",
              "      <td>0.978588</td>\n",
              "      <td>0.931894</td>\n",
              "      <td>-0.949651</td>\n",
              "      <td>0.982580</td>\n",
              "      <td>-0.909394</td>\n",
              "      <td>-0.975542</td>\n",
              "      <td>-0.516218</td>\n",
              "      <td>0.900263</td>\n",
              "      <td>0.956331</td>\n",
              "      <td>0.963730</td>\n",
              "      <td>-0.979445</td>\n",
              "      <td>0.980862</td>\n",
              "      <td>0.976211</td>\n",
              "      <td>0.974258</td>\n",
              "      <td>0.903596</td>\n",
              "      <td>-0.943132</td>\n",
              "      <td>-0.933623</td>\n",
              "      <td>0.953086</td>\n",
              "      <td>-0.977805</td>\n",
              "      <td>-0.980602</td>\n",
              "      <td>-0.981003</td>\n",
              "      <td>0.950210</td>\n",
              "      <td>0.979766</td>\n",
              "      <td>-0.978626</td>\n",
              "      <td>-0.964125</td>\n",
              "      <td>-0.987014</td>\n",
              "      <td>-0.030494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.995235</td>\n",
              "      <td>0.992913</td>\n",
              "      <td>0.856579</td>\n",
              "      <td>0.982416</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.992667</td>\n",
              "      <td>0.991137</td>\n",
              "      <td>-0.993409</td>\n",
              "      <td>0.942710</td>\n",
              "      <td>-0.995418</td>\n",
              "      <td>-0.995803</td>\n",
              "      <td>-0.995573</td>\n",
              "      <td>0.990111</td>\n",
              "      <td>0.978010</td>\n",
              "      <td>0.971864</td>\n",
              "      <td>-0.989313</td>\n",
              "      <td>-0.991722</td>\n",
              "      <td>-0.991216</td>\n",
              "      <td>0.993099</td>\n",
              "      <td>0.676716</td>\n",
              "      <td>0.991352</td>\n",
              "      <td>-0.967804</td>\n",
              "      <td>-0.994611</td>\n",
              "      <td>-0.994951</td>\n",
              "      <td>0.964720</td>\n",
              "      <td>0.966225</td>\n",
              "      <td>0.798080</td>\n",
              "      <td>-0.996245</td>\n",
              "      <td>-0.506720</td>\n",
              "      <td>0.996277</td>\n",
              "      <td>-0.976448</td>\n",
              "      <td>0.995007</td>\n",
              "      <td>-0.994988</td>\n",
              "      <td>-0.995683</td>\n",
              "      <td>0.995487</td>\n",
              "      <td>-0.993218</td>\n",
              "      <td>-0.949190</td>\n",
              "      <td>-0.973025</td>\n",
              "      <td>-0.935321</td>\n",
              "      <td>0.986950</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.993330</td>\n",
              "      <td>-0.923663</td>\n",
              "      <td>-0.994440</td>\n",
              "      <td>-0.953328</td>\n",
              "      <td>0.944216</td>\n",
              "      <td>-0.572653</td>\n",
              "      <td>0.978280</td>\n",
              "      <td>-0.956738</td>\n",
              "      <td>0.954276</td>\n",
              "      <td>0.991439</td>\n",
              "      <td>0.994935</td>\n",
              "      <td>-0.993280</td>\n",
              "      <td>-0.992990</td>\n",
              "      <td>0.995077</td>\n",
              "      <td>0.962443</td>\n",
              "      <td>-0.975477</td>\n",
              "      <td>0.994096</td>\n",
              "      <td>-0.933072</td>\n",
              "      <td>-0.991474</td>\n",
              "      <td>-0.475111</td>\n",
              "      <td>0.929208</td>\n",
              "      <td>0.978447</td>\n",
              "      <td>0.986826</td>\n",
              "      <td>-0.995205</td>\n",
              "      <td>0.995773</td>\n",
              "      <td>0.993778</td>\n",
              "      <td>0.992921</td>\n",
              "      <td>0.909311</td>\n",
              "      <td>-0.962860</td>\n",
              "      <td>-0.942126</td>\n",
              "      <td>0.976979</td>\n",
              "      <td>-0.994614</td>\n",
              "      <td>-0.995662</td>\n",
              "      <td>-0.988855</td>\n",
              "      <td>0.975657</td>\n",
              "      <td>0.995215</td>\n",
              "      <td>-0.993208</td>\n",
              "      <td>-0.987759</td>\n",
              "      <td>-0.995723</td>\n",
              "      <td>-0.018510</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 768 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       765       766       767\n",
              "0  1.000000 -0.993814 -0.872804  ...  0.992500  0.995524  0.043075\n",
              "1 -0.993814  1.000000  0.857110  ... -0.990122 -0.990480 -0.010351\n",
              "2 -0.872804  0.857110  1.000000  ... -0.868185 -0.852478 -0.168928\n",
              "3 -0.977676  0.974137  0.817612  ... -0.964125 -0.987014 -0.030494\n",
              "4 -0.995235  0.992913  0.856579  ... -0.987759 -0.995723 -0.018510\n",
              "\n",
              "[5 rows x 768 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy_KXaHUuKyF"
      },
      "source": [
        "### **Principal Component Analysis(PCA):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBvBynmZy9rs"
      },
      "source": [
        "#initializing  PCA\n",
        "pca = PCA(svd_solver='randomized', random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9ihZy6ty9uS",
        "outputId": "44999a13-b750-44d1-b6ea-d01a5e7771ba"
      },
      "source": [
        "# fiting the help internation data with PCA\n",
        "pca.fit(x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=None, random_state=42,\n",
              "    svd_solver='randomized', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "OQPMZEY3GjLw",
        "outputId": "70987981-44b9-41c2-d8f0-828529ea77cc"
      },
      "source": [
        "# Visualzing the PCA for optimum no of PCs\n",
        "fig = plt.figure(figsize=(20, 10)) \n",
        "x = range(1,x_train.shape[1]+1)\n",
        "plt.xticks(np.arange(min(x), 51, 3.0))\n",
        "sns.lineplot(y=np.cumsum(pca.explained_variance_ratio_[:50]),x = range(1,51))\n",
        "plt.axhline(y=0.99845, color='r', linestyle='--')\n",
        "plt.axvline(x=20, color='b', linestyle='--')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.lines.Line2D at 0x7ff34331c828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPafLMNmFCcs"
      },
      "source": [
        "Out of 768 dimensions,only **20 dimensional data** could explain **99.84% variance** of data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqZLgjmbFBbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc05c6e-3328-4c21-9f8b-aacb50952bef"
      },
      "source": [
        "# we take 11 principal components as it explains 99.8% of variance \n",
        "pca_new= PCA(n_components=20, random_state=42)\n",
        "pca_train= pca_new.fit_transform(x_train)\n",
        "pca_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5728, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "WO_6-ilqF-mK",
        "outputId": "3e628587-8426-4b48-cccf-63ac008cd06e"
      },
      "source": [
        "pca_columns=[]\n",
        "for i in range(1,21):\n",
        "    i=str(i)\n",
        "    value='PCA'+i\n",
        "    pca_columns.append(value)\n",
        "df_train = pd.DataFrame(pca_train, columns=pca_columns,index=x_train.index)\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PCA1</th>\n",
              "      <th>PCA2</th>\n",
              "      <th>PCA3</th>\n",
              "      <th>PCA4</th>\n",
              "      <th>PCA5</th>\n",
              "      <th>PCA6</th>\n",
              "      <th>PCA7</th>\n",
              "      <th>PCA8</th>\n",
              "      <th>PCA9</th>\n",
              "      <th>PCA10</th>\n",
              "      <th>PCA11</th>\n",
              "      <th>PCA12</th>\n",
              "      <th>PCA13</th>\n",
              "      <th>PCA14</th>\n",
              "      <th>PCA15</th>\n",
              "      <th>PCA16</th>\n",
              "      <th>PCA17</th>\n",
              "      <th>PCA18</th>\n",
              "      <th>PCA19</th>\n",
              "      <th>PCA20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.611555</td>\n",
              "      <td>0.374291</td>\n",
              "      <td>-0.027610</td>\n",
              "      <td>-0.190436</td>\n",
              "      <td>-0.468256</td>\n",
              "      <td>-0.453189</td>\n",
              "      <td>-0.031909</td>\n",
              "      <td>0.063972</td>\n",
              "      <td>-0.139020</td>\n",
              "      <td>0.030282</td>\n",
              "      <td>-0.028132</td>\n",
              "      <td>0.017393</td>\n",
              "      <td>0.095144</td>\n",
              "      <td>0.067748</td>\n",
              "      <td>-0.061398</td>\n",
              "      <td>0.073001</td>\n",
              "      <td>-0.094584</td>\n",
              "      <td>0.032299</td>\n",
              "      <td>0.060369</td>\n",
              "      <td>-0.150151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-12.746002</td>\n",
              "      <td>0.433599</td>\n",
              "      <td>-0.619970</td>\n",
              "      <td>0.166478</td>\n",
              "      <td>-0.557112</td>\n",
              "      <td>0.045851</td>\n",
              "      <td>-0.872665</td>\n",
              "      <td>0.050524</td>\n",
              "      <td>-0.161692</td>\n",
              "      <td>0.012113</td>\n",
              "      <td>-0.001802</td>\n",
              "      <td>0.069004</td>\n",
              "      <td>0.032701</td>\n",
              "      <td>0.066070</td>\n",
              "      <td>0.339659</td>\n",
              "      <td>-0.181525</td>\n",
              "      <td>0.129398</td>\n",
              "      <td>-0.000125</td>\n",
              "      <td>0.048145</td>\n",
              "      <td>0.091203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-12.974072</td>\n",
              "      <td>0.094273</td>\n",
              "      <td>-0.165440</td>\n",
              "      <td>0.042125</td>\n",
              "      <td>0.213158</td>\n",
              "      <td>-0.056517</td>\n",
              "      <td>-0.028904</td>\n",
              "      <td>0.008160</td>\n",
              "      <td>-0.044141</td>\n",
              "      <td>-0.003576</td>\n",
              "      <td>-0.017322</td>\n",
              "      <td>0.050320</td>\n",
              "      <td>0.051848</td>\n",
              "      <td>-0.046349</td>\n",
              "      <td>-0.038665</td>\n",
              "      <td>0.036184</td>\n",
              "      <td>-0.025566</td>\n",
              "      <td>-0.000108</td>\n",
              "      <td>0.036850</td>\n",
              "      <td>0.025482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>15.220882</td>\n",
              "      <td>-2.154268</td>\n",
              "      <td>0.265459</td>\n",
              "      <td>0.431031</td>\n",
              "      <td>-0.085601</td>\n",
              "      <td>-0.107140</td>\n",
              "      <td>-0.034160</td>\n",
              "      <td>0.062973</td>\n",
              "      <td>-0.085473</td>\n",
              "      <td>0.126394</td>\n",
              "      <td>-0.068958</td>\n",
              "      <td>0.142078</td>\n",
              "      <td>0.020188</td>\n",
              "      <td>0.034583</td>\n",
              "      <td>0.033741</td>\n",
              "      <td>0.098227</td>\n",
              "      <td>-0.009423</td>\n",
              "      <td>0.017803</td>\n",
              "      <td>-0.043863</td>\n",
              "      <td>-0.020218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-12.983355</td>\n",
              "      <td>0.053846</td>\n",
              "      <td>-0.112174</td>\n",
              "      <td>0.140641</td>\n",
              "      <td>0.018356</td>\n",
              "      <td>0.016067</td>\n",
              "      <td>-0.067726</td>\n",
              "      <td>0.037043</td>\n",
              "      <td>-0.018321</td>\n",
              "      <td>0.086047</td>\n",
              "      <td>0.045006</td>\n",
              "      <td>-0.071414</td>\n",
              "      <td>0.043136</td>\n",
              "      <td>-0.117942</td>\n",
              "      <td>-0.036687</td>\n",
              "      <td>0.078981</td>\n",
              "      <td>0.044350</td>\n",
              "      <td>0.131215</td>\n",
              "      <td>0.095560</td>\n",
              "      <td>-0.013779</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        PCA1      PCA2      PCA3  ...     PCA18     PCA19     PCA20\n",
              "0  14.611555  0.374291 -0.027610  ...  0.032299  0.060369 -0.150151\n",
              "1 -12.746002  0.433599 -0.619970  ... -0.000125  0.048145  0.091203\n",
              "2 -12.974072  0.094273 -0.165440  ... -0.000108  0.036850  0.025482\n",
              "3  15.220882 -2.154268  0.265459  ...  0.017803 -0.043863 -0.020218\n",
              "4 -12.983355  0.053846 -0.112174  ...  0.131215  0.095560 -0.013779\n",
              "\n",
              "[5 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbUzUCfwGK3i",
        "outputId": "51731a3d-6dd1-48e3-aeb0-755f10b91206"
      },
      "source": [
        "#Applying selected components to the test data - 11 components\n",
        "pca_test = pca_new.transform(x_test)\n",
        "pca_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(811, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "CfldfIK_ljOH",
        "outputId": "4ecf91f8-2480-4755-eb01-af294346033a"
      },
      "source": [
        "pca_columns=[]\n",
        "for i in range(1,21):\n",
        "    i=str(i)\n",
        "    value='PCA'+i\n",
        "    pca_columns.append(value)\n",
        "df_test = pd.DataFrame(pca_test, columns=pca_columns,index=x_test.index)\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PCA1</th>\n",
              "      <th>PCA2</th>\n",
              "      <th>PCA3</th>\n",
              "      <th>PCA4</th>\n",
              "      <th>PCA5</th>\n",
              "      <th>PCA6</th>\n",
              "      <th>PCA7</th>\n",
              "      <th>PCA8</th>\n",
              "      <th>PCA9</th>\n",
              "      <th>PCA10</th>\n",
              "      <th>PCA11</th>\n",
              "      <th>PCA12</th>\n",
              "      <th>PCA13</th>\n",
              "      <th>PCA14</th>\n",
              "      <th>PCA15</th>\n",
              "      <th>PCA16</th>\n",
              "      <th>PCA17</th>\n",
              "      <th>PCA18</th>\n",
              "      <th>PCA19</th>\n",
              "      <th>PCA20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-12.994799</td>\n",
              "      <td>0.015741</td>\n",
              "      <td>-0.087510</td>\n",
              "      <td>0.040984</td>\n",
              "      <td>-0.158826</td>\n",
              "      <td>-0.019565</td>\n",
              "      <td>-0.254750</td>\n",
              "      <td>-0.017569</td>\n",
              "      <td>0.022806</td>\n",
              "      <td>-0.074567</td>\n",
              "      <td>-0.113135</td>\n",
              "      <td>0.044530</td>\n",
              "      <td>-0.043701</td>\n",
              "      <td>0.015066</td>\n",
              "      <td>0.215584</td>\n",
              "      <td>-0.138475</td>\n",
              "      <td>0.025714</td>\n",
              "      <td>-0.043675</td>\n",
              "      <td>-0.134477</td>\n",
              "      <td>-0.152025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14.416045</td>\n",
              "      <td>0.162261</td>\n",
              "      <td>0.701503</td>\n",
              "      <td>0.430803</td>\n",
              "      <td>0.359154</td>\n",
              "      <td>0.086245</td>\n",
              "      <td>-0.013670</td>\n",
              "      <td>0.320923</td>\n",
              "      <td>-0.077712</td>\n",
              "      <td>0.389170</td>\n",
              "      <td>-0.197261</td>\n",
              "      <td>-0.152301</td>\n",
              "      <td>-0.152626</td>\n",
              "      <td>-0.014025</td>\n",
              "      <td>0.103110</td>\n",
              "      <td>0.030712</td>\n",
              "      <td>0.081496</td>\n",
              "      <td>-0.049799</td>\n",
              "      <td>0.207770</td>\n",
              "      <td>-0.104858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-13.013250</td>\n",
              "      <td>0.220053</td>\n",
              "      <td>-0.389429</td>\n",
              "      <td>0.158356</td>\n",
              "      <td>0.293556</td>\n",
              "      <td>-0.072333</td>\n",
              "      <td>0.011838</td>\n",
              "      <td>-0.141063</td>\n",
              "      <td>-0.148427</td>\n",
              "      <td>0.033756</td>\n",
              "      <td>0.012854</td>\n",
              "      <td>-0.007482</td>\n",
              "      <td>0.037999</td>\n",
              "      <td>-0.009980</td>\n",
              "      <td>0.056956</td>\n",
              "      <td>-0.020139</td>\n",
              "      <td>-0.065626</td>\n",
              "      <td>0.018651</td>\n",
              "      <td>0.012415</td>\n",
              "      <td>-0.067363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-10.395929</td>\n",
              "      <td>0.693776</td>\n",
              "      <td>1.875468</td>\n",
              "      <td>0.740886</td>\n",
              "      <td>-1.145555</td>\n",
              "      <td>0.744189</td>\n",
              "      <td>0.060122</td>\n",
              "      <td>0.121721</td>\n",
              "      <td>-0.012940</td>\n",
              "      <td>0.204585</td>\n",
              "      <td>-0.055946</td>\n",
              "      <td>0.253422</td>\n",
              "      <td>-0.017891</td>\n",
              "      <td>0.568733</td>\n",
              "      <td>-0.284353</td>\n",
              "      <td>-0.310158</td>\n",
              "      <td>0.172016</td>\n",
              "      <td>0.056229</td>\n",
              "      <td>0.270779</td>\n",
              "      <td>0.105079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-12.751725</td>\n",
              "      <td>0.012231</td>\n",
              "      <td>0.314402</td>\n",
              "      <td>0.205846</td>\n",
              "      <td>-0.469746</td>\n",
              "      <td>0.292758</td>\n",
              "      <td>0.449456</td>\n",
              "      <td>-0.026206</td>\n",
              "      <td>-0.133566</td>\n",
              "      <td>-0.027286</td>\n",
              "      <td>0.048049</td>\n",
              "      <td>-0.073949</td>\n",
              "      <td>-0.058254</td>\n",
              "      <td>0.056493</td>\n",
              "      <td>0.055347</td>\n",
              "      <td>-0.001235</td>\n",
              "      <td>0.014248</td>\n",
              "      <td>-0.100053</td>\n",
              "      <td>-0.041855</td>\n",
              "      <td>-0.070072</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        PCA1      PCA2      PCA3  ...     PCA18     PCA19     PCA20\n",
              "0 -12.994799  0.015741 -0.087510  ... -0.043675 -0.134477 -0.152025\n",
              "1  14.416045  0.162261  0.701503  ... -0.049799  0.207770 -0.104858\n",
              "2 -13.013250  0.220053 -0.389429  ...  0.018651  0.012415 -0.067363\n",
              "3 -10.395929  0.693776  1.875468  ...  0.056229  0.270779  0.105079\n",
              "4 -12.751725  0.012231  0.314402  ... -0.100053 -0.041855 -0.070072\n",
              "\n",
              "[5 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3DvgMavkTwZ"
      },
      "source": [
        "#Initializing MinMaxScalar\n",
        "\n",
        "mms = MinMaxScaler()\n",
        "#scaling the Training dataset\n",
        "df_train[df_train.columns] = mms.fit_transform(df_train[df_train.columns])\n",
        "\n",
        "#scaling the Test dataset\n",
        "df_test[df_test.columns] = mms.transform(df_test[df_test.columns])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTemJJn76DRD"
      },
      "source": [
        "### **GRID SEARCH :** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApaUNxq9vUR6"
      },
      "source": [
        "def grid_search(algo,x,y,param_grid):\n",
        "  \"\"\"\n",
        "  input   : algo : classification algorithm \n",
        "             x : x_train \n",
        "             y : y_train \n",
        "  prints  : best hyperparameters \n",
        "  \"\"\"\n",
        "  # Instantiate the grid search model\n",
        "  grid_search = GridSearchCV(estimator = algo, param_grid = param_grid, \n",
        "                          cv = 3, n_jobs = -1,verbose = 1)\n",
        "  # Fit the grid search to the data\n",
        "  grid_search.fit(x, y)\n",
        "  print('\\n\\nWe can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsdIqKE2tFQL"
      },
      "source": [
        "### **SVM WITHOUT PCA :** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPK8xteQvbmo",
        "outputId": "085ac7f5-d76c-478a-9d1c-9e73422f4b4d"
      },
      "source": [
        "# Set the parameters by cross-validation\n",
        "svc = SVC()\n",
        "param_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
        "                     'C': [1, 10, 100, 1000]}]\n",
        "grid_search(svc,x_train,y_train,param_grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "We can get accuracy of 1.0 using {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    2.2s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDV0-X-7vck6",
        "outputId": "1343a7c8-2fd3-43b2-a227-e31347f862a5"
      },
      "source": [
        "svc_nonpca=SVC(C=1,gamma=0.001,kernel='rbf')\n",
        "\n",
        "# fit\n",
        "svc_nonpca.fit(x_train,y_train)\n",
        "\n",
        "# predict\n",
        "y_pred = pd.Series(svc_nonpca.predict(x_test))\n",
        "\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9149198520345253"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAZjJmLjvjzi",
        "outputId": "f97d1cb1-ca90-4427-8d74-067a75138f54"
      },
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92       435\n",
            "           1       0.92      0.90      0.91       376\n",
            "\n",
            "    accuracy                           0.91       811\n",
            "   macro avg       0.92      0.91      0.91       811\n",
            "weighted avg       0.91      0.91      0.91       811\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGMl3NByvk3V"
      },
      "source": [
        "### **SVM WITH PCA :** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTR_maRPy9w3",
        "outputId": "c0737185-4891-4e49-c655-d45511309565"
      },
      "source": [
        "# Set the parameters by cross-validation\n",
        "svc = SVC()\n",
        "param_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
        "                     'C': [1, 10, 100, 1000]}]\n",
        "grid_search(svc,df_train,y_train,param_grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    5.8s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "We can get accuracy of 1.0 using {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iHK0VvzvzcM",
        "outputId": "179aecfc-6c5b-4868-9ff1-93185f346c4c"
      },
      "source": [
        "svc_pca=SVC(C=1,gamma=0.001,kernel='rbf')\n",
        "\n",
        "# fit\n",
        "svc_pca.fit(df_train,y_train)\n",
        "\n",
        "# predict\n",
        "y_pred = pd.Series(svc_pca.predict(df_test))\n",
        "\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9149198520345253"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_InYe6kv445",
        "outputId": "733c4f98-f28b-482f-c560-cd1c0c83660d"
      },
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92       435\n",
            "           1       0.92      0.90      0.91       376\n",
            "\n",
            "    accuracy                           0.91       811\n",
            "   macro avg       0.92      0.91      0.91       811\n",
            "weighted avg       0.91      0.91      0.91       811\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F42JGRbkwfzm"
      },
      "source": [
        "### **RANDOM FOREST WITHOUT PCA:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3scYn_HX4Vv_",
        "outputId": "c39b5909-4eab-4589-cbcb-35261d9660f6"
      },
      "source": [
        "# Create a random forest model\n",
        "rf = RandomForestClassifier()\n",
        "# Create the parameter grid based on the results of random search \n",
        "param_grid = {\n",
        "    'max_depth': [2,4,6],\n",
        "    'min_samples_leaf': range(50, 150, 50),\n",
        "    'min_samples_split': range(100, 400, 100),\n",
        "    'n_estimators': [50,100,150], \n",
        "    'max_features': [3,5,7]\n",
        "}\n",
        "grid_search(rf,x_train,y_train,param_grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    9.1s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   44.6s\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=-1)]: Done 486 out of 486 | elapsed:  1.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "We can get accuracy of 1.0 using {'max_depth': 2, 'max_features': 3, 'min_samples_leaf': 50, 'min_samples_split': 100, 'n_estimators': 50}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2cAwa3h4VyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef4682d2-a23b-4bca-cd3b-afb97c5aa86a"
      },
      "source": [
        "rfc_nonpca = RandomForestClassifier(bootstrap=True,\n",
        "                             max_depth=2,\n",
        "                             min_samples_leaf=50, \n",
        "                             min_samples_split=100,\n",
        "                             max_features=3,\n",
        "                             n_estimators=50)\n",
        "\n",
        "# fit\n",
        "rfc_nonpca.fit(x_train,y_train)\n",
        "\n",
        "# predict\n",
        "y_pred = pd.Series(rfc_nonpca.predict(x_test))\n",
        "y_pred_prob = pd.Series(rfc_nonpca.predict_proba(x_test)[:,1])\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9149198520345253"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duvxOtocw7aV",
        "outputId": "3fe30059-ee47-41c3-ed74-a0510bc4be1e"
      },
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.92       435\n",
            "           1       0.91      0.90      0.91       376\n",
            "\n",
            "    accuracy                           0.91       811\n",
            "   macro avg       0.91      0.91      0.91       811\n",
            "weighted avg       0.91      0.91      0.91       811\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD5JuNpZxGVn"
      },
      "source": [
        "### **RANDOM FOREST WITH PCA :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGBCo2l34V0l",
        "outputId": "8d24c5a0-7f4e-4708-8d6f-f1b5173407d2"
      },
      "source": [
        "# Create a random forest model\n",
        "rf = RandomForestClassifier()\n",
        "# Create the parameter grid based on the results of random search \n",
        "param_grid = {\n",
        "    'max_depth': [2,4,6],\n",
        "    'min_samples_leaf': range(50, 150, 50),\n",
        "    'min_samples_split': range(100, 400, 100),\n",
        "    'n_estimators': [50,100,150], \n",
        "    'max_features': [3,5,7]\n",
        "}\n",
        "grid_search(rf,df_train,y_train,param_grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   10.7s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   57.5s\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=-1)]: Done 486 out of 486 | elapsed:  2.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "We can get accuracy of 1.0 using {'max_depth': 2, 'max_features': 3, 'min_samples_leaf': 50, 'min_samples_split': 100, 'n_estimators': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHSRY35R4V3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3adc8b32-7ecb-4e0b-b5e4-e39929e2b0fc"
      },
      "source": [
        "rfc_pca = RandomForestClassifier(bootstrap=True,\n",
        "                             max_depth=2,\n",
        "                             min_samples_leaf=50, \n",
        "                             min_samples_split=100,\n",
        "                             max_features=3,\n",
        "                             n_estimators=50)\n",
        "# fit\n",
        "rfc_pca.fit(df_train,y_train)\n",
        "\n",
        "# predict\n",
        "y_pred = pd.Series(rfc_pca.predict(df_test))\n",
        "y_pred_prob = pd.Series(rfc_pca.predict_proba(df_test)[:,1])\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9149198520345253"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b-osnH2_a6F",
        "outputId": "422a8c2b-1b0f-4b89-967f-d2bcf1168f35"
      },
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92       435\n",
            "           1       0.92      0.90      0.91       376\n",
            "\n",
            "    accuracy                           0.91       811\n",
            "   macro avg       0.92      0.91      0.91       811\n",
            "weighted avg       0.91      0.91      0.91       811\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUZlGnUvxujJ"
      },
      "source": [
        "### **XGBOOST WITHOUT PCA :**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6I6vMUk4V6f",
        "outputId": "10146c69-21bf-455e-805e-935b02be3ecd"
      },
      "source": [
        "# Create a XGBoost model\n",
        "xgb = XGBClassifier()\n",
        "\n",
        "# Create the parameter grid based on the results of random search \n",
        "param_grid = {\n",
        "        'min_child_weight': [1, 5, 10],\n",
        "        'gamma': [0.5, 1],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "        'max_depth': [3]\n",
        "}\n",
        "\n",
        "grid_search(xgb,x_train,y_train,param_grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.8min\n",
            "[Parallel(n_jobs=-1)]: Done 162 out of 162 | elapsed: 11.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "We can get accuracy of 1.0 using {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "topbbwtY4V86",
        "outputId": "2314df4a-a4ef-4363-cc01-2cacc98c1588"
      },
      "source": [
        "xgb_nonpca =  XGBClassifier(max_depth=3,colsample_bytree=0.6,\n",
        "                             gamma=0.5, \n",
        "                             min_child_weight=1,\n",
        "                             subsample=0.6)\n",
        "# fit\n",
        "xgb_nonpca.fit(x_train,y_train)\n",
        "\n",
        "# predict\n",
        "y_pred = pd.Series(xgb_nonpca.predict(x_test))\n",
        "y_pred_prob = pd.Series(xgb_nonpca.predict_proba(x_test)[:,1])\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9198520345252774"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RXwhPwnx-19",
        "outputId": "0d9ee51b-371e-4126-e2a3-40ef6c5f2718"
      },
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.94      0.93       435\n",
            "           1       0.92      0.90      0.91       376\n",
            "\n",
            "    accuracy                           0.92       811\n",
            "   macro avg       0.92      0.92      0.92       811\n",
            "weighted avg       0.92      0.92      0.92       811\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8gtC5AxyGlj"
      },
      "source": [
        "### **XGBOOST WITH PCA:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtKJONkfyF4s",
        "outputId": "e4dd6003-58d7-431d-e85d-e90a1484ad51"
      },
      "source": [
        "# Create a XGBoost model\n",
        "xgb = XGBClassifier()\n",
        "\n",
        "# Create the parameter grid based on the results of random search \n",
        "param_grid = {\n",
        "        'min_child_weight': [1, 5, 10],\n",
        "        'gamma': [0.5, 1],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "        'max_depth': [3]\n",
        "}\n",
        "\n",
        "grid_search(xgb,df_train,y_train,param_grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    8.6s\n",
            "[Parallel(n_jobs=-1)]: Done 162 out of 162 | elapsed:   29.6s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "We can get accuracy of 1.0 using {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70rszQKx4V_4",
        "outputId": "0f37f746-8bac-4894-b7d0-8101c7557558"
      },
      "source": [
        "xgb_pca =  XGBClassifier(max_depth=3,colsample_bytree=0.6,\n",
        "                             gamma=0.5, \n",
        "                             min_child_weight=1,\n",
        "                             subsample=0.6)\n",
        "# fit\n",
        "xgb_pca.fit(df_train,y_train)\n",
        "\n",
        "# predict\n",
        "y_pred = pd.Series(xgb_pca.predict(df_test))\n",
        "y_pred_prob = pd.Series(xgb_pca.predict_proba(df_test)[:,1])\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9149198520345253"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOdf_H0P4WCp",
        "outputId": "f21d62c4-f0d9-42ea-c45e-27a8d4fd64ed"
      },
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92       435\n",
            "           1       0.92      0.90      0.91       376\n",
            "\n",
            "    accuracy                           0.91       811\n",
            "   macro avg       0.92      0.91      0.91       811\n",
            "weighted avg       0.91      0.91      0.91       811\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79R-DMuzyWiW"
      },
      "source": [
        "### **GBDT WITHOUT PCA :**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h87VcgkB4WFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "580abbd6-32e9-489d-e3e2-1b0d3b77bde4"
      },
      "source": [
        "# grid search to find the optimal hyperparameters\n",
        "\n",
        "# Create a GBDT model\n",
        "gbdt=GradientBoostingClassifier()\n",
        "\n",
        "# Create the parameter grid based on the results of random search \n",
        "param_grid = {\n",
        "     'max_depth': [2,4,6],\n",
        "    'min_samples_leaf': range(50, 150, 50),\n",
        "    'min_samples_split': range(100, 400, 100),\n",
        "    'n_estimators': [50,100,150], \n",
        "    'max_features': [3,5,7]\n",
        "}\n",
        "grid_search(gbdt,x_train,y_train,param_grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    6.3s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   32.1s\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=-1)]: Done 486 out of 486 | elapsed:  1.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "We can get accuracy of 1.0 using {'max_depth': 2, 'max_features': 3, 'min_samples_leaf': 50, 'min_samples_split': 100, 'n_estimators': 50}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WgMWNp24WH2",
        "outputId": "d9de5740-759e-4739-e254-708a1f1d10f5"
      },
      "source": [
        "gbdt_nonpca = GradientBoostingClassifier(max_depth=2,\n",
        "                             min_samples_leaf=50, \n",
        "                             min_samples_split=100,\n",
        "                             max_features=3,\n",
        "                             n_estimators=50)\n",
        "# fit\n",
        "gbdt_nonpca.fit(x_train,y_train)\n",
        "\n",
        "# predict\n",
        "y_pred = pd.Series(gbdt_nonpca.predict(x_test))\n",
        "y_pred_prob = pd.Series(gbdt_nonpca.predict_proba(x_test)[:,1])\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9136868064118372"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AXUNjKw4WKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1bd0ae9-0cc8-48e1-eded-fe2964b09f34"
      },
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92       435\n",
            "           1       0.91      0.90      0.91       376\n",
            "\n",
            "    accuracy                           0.91       811\n",
            "   macro avg       0.91      0.91      0.91       811\n",
            "weighted avg       0.91      0.91      0.91       811\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY07i2xMyjln"
      },
      "source": [
        "### **GBDT WITH PCA :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDbR-GE04WNF",
        "outputId": "c918c1e0-8b07-457b-9d7a-481bcb05982a"
      },
      "source": [
        "# grid search to find the optimal hyperparameters\n",
        "\n",
        "# Create a GBDT model\n",
        "gbdt=GradientBoostingClassifier()\n",
        "\n",
        "# Create the parameter grid based on the results of random search \n",
        "param_grid = {\n",
        "     'max_depth': [2,4,6],\n",
        "    'min_samples_leaf': range(50, 150, 50),\n",
        "    'min_samples_split': range(100, 400, 100),\n",
        "    'n_estimators': [50,100,150], \n",
        "    'max_features': [3,5,7]\n",
        "}\n",
        "grid_search(gbdt,df_train,y_train,param_grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   11.0s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  3.5min\n",
            "[Parallel(n_jobs=-1)]: Done 486 out of 486 | elapsed:  4.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "We can get accuracy of 1.0 using {'max_depth': 2, 'max_features': 3, 'min_samples_leaf': 50, 'min_samples_split': 100, 'n_estimators': 50}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJXp1NZO4WPx",
        "outputId": "27e13349-9395-4b38-a2bf-123dd3e534c6"
      },
      "source": [
        "gbdt_pca = GradientBoostingClassifier(max_depth=2,\n",
        "                             min_samples_leaf=50, \n",
        "                             min_samples_split=100,\n",
        "                             max_features=3,\n",
        "                             n_estimators=50)\n",
        "# fit\n",
        "gbdt_pca.fit(df_train,y_train)\n",
        "\n",
        "# predict\n",
        "y_pred = pd.Series(gbdt_pca.predict(df_test))\n",
        "y_pred_prob = pd.Series(gbdt_pca.predict_proba(df_test)[:,1])\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9149198520345253"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76ymbtXm4WSW",
        "outputId": "bc37d859-72ac-43cb-86be-e3e555f5e48a"
      },
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92       435\n",
            "           1       0.92      0.90      0.91       376\n",
            "\n",
            "    accuracy                           0.91       811\n",
            "   macro avg       0.92      0.91      0.91       811\n",
            "weighted avg       0.91      0.91      0.91       811\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}